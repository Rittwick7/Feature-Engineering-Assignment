{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a parameter?\n",
        "  - In feature engineering, a parameter refers to a value or setting that is determined or chosen during the process of creating or transforming features. These parameters guide how the feature engineering techniques are applied to the raw data.\n",
        "\n",
        "2. What is correlation?\n",
        "  - Correlation is a statistical measure that quantifies the linear relationship between two variables, helping to identify redundant features, assess a feature's predictive power, and inform the selection or transformation of features to improve a machine learning model's performance and stability.\n",
        "  What does negative correlation mean?\n",
        "  - Negative correlation describes an inverse relationship between two variables, meaning that when one variable increases, the other tends to decrease, and vice versa.\n",
        "\n",
        "3. Define Machine Learning. What are the main components in Machine Learning?\n",
        "  - Machine learning is a branch of Artificial Intelligence that enables computer systems to learn from data and improve their performance on tasks over time without being explicitly programmed.\n",
        "   The key elements of machine learning are Data, Models, Algorithms, Training, and Evaluation.\n",
        "\n",
        "4. How does loss value help in determining whether the model is good or not?\n",
        "  - A lower loss value indicates a better-performing machine learning model, as it signifies a smaller discrepancy between the model's predictions and the actual target values. During training, models adjust their internal parameters to minimize this loss, with the goal being to reach the lowest possible loss value, demonstrating accurate and reliable predictions.\n",
        "\n",
        "5. What are continuous and categorical variables?\n",
        "  - Categorical variables represent distinct groups or categories, such as hair color or gender, which are qualitative and lack inherent numerical order, though some categories may have a ranking (ordinal). Continuous variables are numerical and can take on any value within a given range, representing measurements like a person's height, weight, or temperature.\n",
        "\n",
        "6. How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "  - To handle categorical variables in Machine Learning, convert them into a numerical format that algorithms can process, using techniques like One-Hot Encoding for nominal (no order) data, Ordinal Encoding for ordered data, and other methods such as Target Encoding or Frequency Encoding to capture relationships between categories and the target variable.   \n",
        "  What are Categorical Data Encoding Methods | Binary EncodingCommon techniques for handling categorical variables in Machine Learning include One-Hot Encoding, Label Encoding, Ordinal Encoding, Target Encoding, and Frequency Encoding.\n",
        "\n",
        "7. What do you mean by training and testing a dataset?\n",
        "  - In machine learning, datasets are typically split into two subsets: training and testing data. The training data is used to train the machine learning algorithm. The testing data is used to evaluate the accuracy of the trained algorithm.\n",
        "\n",
        "8. What is sklearn.preprocessing?\n",
        "  - The sklearn.preprocessing module in scikit-learn provides a collection of utility functions and transformer classes designed to prepare raw feature vectors for use with machine learning estimators. This preparation, known as data preprocessing, is a crucial step in the machine learning pipeline as it significantly impacts the performance and stability of models.\n",
        "\n",
        "9. What is a Test set?\n",
        "  - A test set is a pre-defined group of test cases or data used to evaluate the performance of a system or model.\n",
        "\n",
        "10. How do we split data for model fitting (training and testing) in Python?\n",
        "  - Data splitting for model fitting (training and testing) in Python is commonly achieved using the train_test_split function from the sklearn.model_selection module. This function allows for a straightforward division of your dataset into training and testing subsets.\n",
        "    How do you approach a Machine Learning problem?\n",
        "  - General steps to follow in a Machine Learning Problem ...To approach a machine learning problem, first, clearly define the problem and goal, then collect and thoroughly prepare the data through cleaning, preprocessing, and feature engineering. Next, select an appropriate ML model, train it on the data, and evaluate its performance. Finally, deploy the model for real-world use and engage in an iterative process of monitoring, maintenance, and refinement to improve performance over time.\n",
        "\n",
        "11. Why do we have to perform EDA before fitting a model to the data?\n",
        "  - EDA is essential before fitting a model because it helps understand data quality, identify patterns and relationships, detect outliers and missing values, inform feature engineering, and select appropriate models. Performing EDA beforehand prevents data leakage, ensures robust models, and leads to more accurate and reliable results, ultimately helping to validate if the data is suitable for the intended task.\n",
        "\n",
        "12. What is correlation?\n",
        "  - A correlation is a statistical measure of the strength and direction of the relationship between two variables, indicating how much they tend to change together. A positive correlation shows variables moving in the same direction, a negative correlation shows them moving in opposite directions, and zero correlation indicates no relationship.\n",
        "\n",
        "13. What does negative correlation mean?\n",
        "  - Negative correlation describes an inverse relationship between two variables, meaning that when one variable increases, the other tends to decrease, and vice versa. This relationship is often called an inverse correlation.\n",
        "\n",
        "14. How can you find correlation between variables in Python?\n",
        "  - Finding the correlation between variables in Python can be accomplished using libraries like NumPy, SciPy, and Pandas. The choice of method depends on the type of variables (continuous, categorical) and the desired correlation measure (Pearson, Spearman, Kendall).\n",
        "\n",
        "15. What is causation? Explain difference between correlation and causation with an example.\n",
        "  - Causation is when one event or variable directly leads to another event or variable, creating a cause-and-effect relationship. Correlation, on the other hand, simply means two variables are related or show a similar pattern, but one does not necessarily cause the other. For example, there is a strong correlation between the number of ice creams sold and the number of sunburns, as both increase in the summer.\n",
        "\n",
        "16. What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "  - An optimizer is an algorithm or method used in machine learning and deep learning to adjust the parameters (weights and biases) of a model during the training process. Its primary goal is to minimize a designated loss function, which quantifies the discrepancy between the model's predictions and the actual target values. By iteratively updating the parameters in the direction that reduces this loss, optimizers guide the model towards finding the optimal set of parameters that yield the best performance.\n",
        "Different Types of Optimizers:\n",
        "1. Gradient Descent (GD):\n",
        "This is the most basic optimization algorithm. It calculates the gradient of the loss function with respect to all parameters in the entire training dataset and updates the parameters in the opposite direction of the gradient.\n",
        "Example: Imagine a simple linear regression model where you want to find the best line to fit a set of data points. Gradient Descent would calculate the average error across all data points and adjust the slope and intercept of the line to reduce that average error.\n",
        "2. Stochastic Gradient Descent (SGD):\n",
        "Unlike Gradient Descent, SGD updates the parameters after processing each individual training example. This makes it faster for large datasets but can lead to more noisy updates.\n",
        "Example: In the same linear regression scenario, SGD would pick one data point at a time, calculate the error for that single point, and immediately adjust the line's parameters to minimize that specific error.\n",
        "3. Mini-Batch Gradient Descent:\n",
        "This method offers a compromise between GD and SGD. It updates parameters after processing a small \"mini-batch\" of training examples, providing a balance between computational efficiency and stability.\n",
        "Example: Instead of one data point or all data points, Mini-Batch Gradient Descent would take a small group of data points (e.g., 32 or 64) at a time, calculate the average error for that group, and then update the line's parameters.\n",
        "4. Adam (Adaptive Moment Estimation):\n",
        "Adam is an adaptive learning rate optimizer that combines the advantages of RMSprop and AdaGrad. It computes adaptive learning rates for each parameter based on estimates of first and second moments of the gradients.\n",
        "Example: When training a neural network for image classification, Adam would dynamically adjust the learning rate for each weight and bias based on its historical gradients, allowing for faster and more stable convergence, especially in complex architectures.\n",
        "5. RMSprop (Root Mean Square Propagation):\n",
        "RMSprop is another adaptive learning rate optimizer that divides the learning rate by an exponentially decaying average of squared gradients. This helps to accelerate convergence in the direction of the minimum and dampens oscillations.\n",
        "Example: In a recurrent neural network (RNN) for natural language processing, RMSprop would adjust the learning rate for different layers based on the magnitude of their respective gradients, preventing vanishing or exploding gradients.\n",
        "\n",
        "17. What is sklearn.linear_model?\n",
        "  - sklearn.linear_model is a module within the scikit-learn (sklearn) library in Python, specifically designed to implement various linear models for both regression and classification tasks in machine learning.\n",
        "\n",
        "18. What does model.fit() do? What arguments must be given?\n",
        "  -\n",
        "\n",
        "19. What does model.predict() do? What arguments must be given?\n",
        "  -\n",
        "\n",
        "20. What are continuous and categorical variables?\n",
        "  - Categorical variables represent distinct groups or categories, such as hair color or gender, which are qualitative and lack inherent numerical order, though some categories may have a ranking (ordinal). Continuous variables are numerical and can take on any value within a given range, representing measurements like a person's height, weight, or temperature.  \n",
        "\n",
        "21. What is feature scaling? How does it help in Machine Learning?\n",
        "  - Feature scaling is the process of transforming features (variables) in a dataset to a common scale or range, such as normalizing them to 0-1 or standardizing them to a mean of 0 and a standard deviation of 1. This technique helps machine learning algorithms perform better by preventing features with larger ranges from disproportionately influencing the model, ensuring all features contribute equally.\n",
        "\n",
        "22. How do we perform scaling in Python?\n",
        "  - Scaling in Python, especially for machine learning, typically involves transforming numerical features to a common range or distribution. This is done to prevent features with larger values from dominating the learning process and to improve the performance of certain algorithms. The sklearn.preprocessing module in scikit-learn provides various methods for this.\n",
        "\n",
        "23. What is sklearn.preprocessing?\n",
        "  - sklearn.preprocessing is a module within the scikit-learn library in Python that provides a collection of utility functions and transformer classes designed for data preprocessing. Data preprocessing is a crucial step in machine learning, involving the transformation of raw data into a format that is more suitable and effective for machine learning algorithms.\n",
        "\n",
        "24. How do we split data for model fitting (training and testing) in Python?\n",
        "  - Data splitting for model fitting (training and testing) in Python is commonly achieved using the train_test_split function from the sklearn.model_selection module. This function allows for a straightforward division of your dataset into training and testing subsets.\n",
        "       from sklearn.model_selection import train_test_split\n",
        "\n",
        "25. Explain data encoding?\n",
        "  - Data encoding is the process of converting data into a specific format, character set, or signal for storage, transmission, or processing by computer systems and algorithms. It ensures data compatibility, efficiency, and integrity, transforming it from a raw, unusable form into one that can be easily understood and manipulated.\n",
        "      "
      ],
      "metadata": {
        "id": "MUChVuVehyQ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***The End***"
      ],
      "metadata": {
        "id": "4jH1EqVnsUn3"
      }
    }
  ]
}